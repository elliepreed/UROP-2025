!pip install transformers huggingface_hub pandas torch pyarrow --quiet

import os
import pandas as pd
import torch
from glob import glob
from huggingface_hub import hf_hub_download, login
from transformers import AutoTokenizer, AutoModelForCausalLM

# üîê Login to Hugging Face (will prompt for token if needed)
login()

# ============================================
# Step 1: Download BLiMP dataset - one subset for each phenomenon (12)
# ============================================
features = [
    "adpositional_phrases",
    "adverbial_modification",
    "anaphor_agreement",
    "argument_structure",
    "auxiliaries",
    "binding_principle_a",
    "complementive",
    "crossing_dependencies",
    "determiners",
    "extraposition",
    "finite_argument_clause",
    "infinitival_argument_clause",
    "nominalization",
    "parasitic_gaps",
    "passive",
    "quantifiers",
    "r_words",
    "relativization",
    "topicalization",
    "verb_second",
    "wh_movement",
    "wh_movement_restrictions"
]


dfs = []

for feature in features:
    filename = f"{feature}-train-00000-of-00001.parquet"
    download_path = hf_hub_download(
        repo_id="juletxara/blimp-nl",
        filename=filename,
        repo_type="dataset",
        cache_dir="hf_cache",
    )
    df = pd.read_parquet(download_path)
    dfs.append(df)

nld_df = pd.concat(dfs, ignore_index=True)

# Save to the BLIMP subdirectory
base_dir = "final_pairs"
phenomenon = "demo_phenomenon"
output_subdir = f"{base_dir}/{phenomenon}/BLIMP"
os.makedirs(output_subdir, exist_ok=True)

nld_condition = "condition1.tsv"
output_path = f"{output_subdir}/{nld_condition}"
nld_df.to_csv(output_path, sep="\t", index=False)

print("‚úÖ BLiMP-NL evaluation file saved to:", output_path)

# ============================================
# Step 2: Evaluation setup
# ============================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def compute_sentence_nll_batch(sentences, model, tokenizer, device, batch_size=16):
    """Compute negative log-likelihoods for sentences in batches."""
    all_nlls = []

    for i in range(0, len(sentences), batch_size):
        batch_sentences = sentences[i:i+batch_size]
        inputs = tokenizer(batch_sentences, return_tensors="pt", padding=True, truncation=True)
        input_ids = inputs["input_ids"].to(device)
        attention_mask = inputs["attention_mask"].to(device)

        with torch.no_grad():
            outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)
            shift_logits = outputs.logits[..., :-1, :].contiguous()
            shift_labels = input_ids[..., 1:].contiguous()

            batch_nlls = []
            for j in range(len(batch_sentences)):
                valid_length = attention_mask[j].sum().item() - 1
                if valid_length > 0:
                    sentence_logits = shift_logits[j, :valid_length]
                    sentence_labels = shift_labels[j, :valid_length]
                    loss_fct = torch.nn.CrossEntropyLoss()
                    nll = loss_fct(sentence_logits, sentence_labels).item()
                    batch_nlls.append(nll)
                else:
                    batch_nlls.append(float('inf'))
            all_nlls.extend(batch_nlls)

    return all_nlls

def score_tse_batch(df, model, tokenizer, device):
    correct_col = "sentence_good"
    wrong_col = "sentence_bad"
    correct_sentences = df[correct_col].tolist()
    wrong_sentences = df[wrong_col].tolist()

    correct_nlls = compute_sentence_nll_batch(correct_sentences, model, tokenizer, device)
    wrong_nlls = compute_sentence_nll_batch(wrong_sentences, model, tokenizer, device)

    df = df.copy()
    df['correct_nll'] = correct_nlls
    df['wrong_nll'] = wrong_nlls
    return df

# ============================================
# Step 3: Iterate over model checkpoints
# ============================================
checkpoints = [0, 10000, 20000, 30000, 40000, 50000, 64000, 64010, 64020, 64030, 64040,
               64050, 64060, 64070, 64080, 64090, 64100, 64110, 64120, 64130, 64140,
               64150, 64160, 64170, 64180, 64190, 64200, 64300, 64400, 64500, 64600,
               64700, 64800, 64900, 65000, 66000, 67000, 68000, 69000, 70000, 80000,
               90000, 100000, 110000, 120000, 128000]

base_model = "catherinearnett/B-GPT_en_el_simultaneous"
pair_files = glob(f"{base_dir}/**/*.tsv", recursive=True)
results_dir = os.path.join("model_results", base_model.replace("/", "_"))
os.makedirs(results_dir, exist_ok=True)

for ckpt in checkpoints:
    revision = str(ckpt)  # e.g., "50000"
    print(f"\nüîé Evaluating checkpoint at step {revision}")

    try:
        tokenizer = AutoTokenizer.from_pretrained(base_model, revision=revision)
        model = AutoModelForCausalLM.from_pretrained(base_model, revision=revision)
        model.to(device)
        model.eval()
    except Exception as e:
        print(f"‚ùå Failed to load checkpoint {revision}: {e}")
        continue

    # ‚Ä¶ then compute NLLs, accuracy, save results as before ‚Ä¶

    for fn in sorted(pair_files):
        print(f"‚Üí Scoring file: {fn}")
        df = pd.read_csv(fn, sep="\t")

        # Compute NLLs and accuracy
        df_with_scores = score_tse_batch(df, model, tokenizer, device)
        correct_better = (df_with_scores['correct_nll'] < df_with_scores['wrong_nll']).sum()
        total = len(df_with_scores)
        accuracy = correct_better / total
        print(f"‚úÖ Accuracy @ ckpt-{ckpt}: {accuracy * 100:.2f}% ({correct_better}/{total})")

        # Save results
        phenomenon_lang = fn.split(os.sep)[-3:-1]
        score_fn = os.path.join(
            results_dir,
            f"{'_'.join(phenomenon_lang)}_ckpt{ckpt}_{os.path.basename(fn)}"
        )
        df_with_scores.to_csv(score_fn, sep="\t", index=False)
        print(f"üìÑ Saved to: {score_fn}")

print("\n‚úÖ All checkpoints evaluated!")
